{"componentChunkName":"component---src-templates-blog-posts-tsx","path":"/posts/javascript/","result":{"pageContext":{"keyword":"JavaScript","data":[{"id":"f49d0a2d-3925-5ae5-8320-57422f8b715f","excerpt":"When implementing applications, we often need to communicate with external services via APIs. In such cases, it’s crucial to ensure that the…","body":"\nWhen implementing applications, we often need to communicate with external services via APIs. In such cases, it's crucial to ensure that the data received from these APIs is valid and conforms to the expected format. It's essential for maintaining the integrity and functionality of various systems.\n\n## What is Zod\n\n[Zod](https://github.com/colinhacks/zod) is a TypeScript-first schema declaration and validation library. It provides an elegant and expressive syntax for defining data schemas and validating data against those schemas in the runtime. Here's a simple example using TypeScript:\n\n```ts\nimport { z } from 'zod';\n\nconst userSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  login: z.string(),\n  createdAt: z.string().datetime(),\n  deletedAt: z.string().datetime().nullable(),\n});\n\nconst userData = {\n  id: \"3f740a80-0af0-4976-9bad-db83b15c7bf7\",\n  email: \"jan.kowalski@example.com\",\n  login: \"jan.kowalski\",\n  createdAt: \"2020-01-01T00:00:00Z\",\n  deletedAt: null,\n};\n\ntry {\n  const validatedUser = userSchema.parse(userData);\n\n  console.log(validatedUser);\n} catch (error) {\n  console.error(error.errors);\n}\n```\n\nIn this example, `userSchema` defines a schema for user data, specifying the expected types and constraints for each field. The `parse` method is then used to validate the `userData` object against the schema. If validation fails, an error is thrown with details about the validation errors.\n\n## Why use Zod\n\n- **Type Safety:** Zod integrates seamlessly with TypeScript, providing strong type checking at compile-time. This helps to catch potential issues early in the development process.\n\n- **Error Reporting:** When validation fails in the run-time, Zod provides detailed error messages, including information about the specific fields that didn't pass validation. This aids in diagnosing and fixing issues efficiently.\n\n- **Readability and Expressiveness:** Zod's syntax is clean and expressive, making it easy to define and understand complex data structures. This enhances code readability and maintainability.\n\n- **Flexibility:** Zod allows you to create sophisticated validation rules, including custom validation functions, conditional validation, and more. This flexibility is valuable when dealing with diverse and evolving data structures.\n\n## How to use Zod\n\nIn a real application, I'd encourage to encapsulate the utilization of Zod within a generic helper function. Let's examine a refined implementation:\n\n```ts\n// api/validator.ts\nimport { z, ZodIssue } from \"zod\";\n\ninterface ValidateConfig<T extends z.ZodTypeAny> {\n  dto: unknown;\n  schema: T;\n  schemaName: string;\n}\n\nexport function validateSchema<T extends z.ZodTypeAny>(\n  config: ValidateConfig<T>\n): z.infer<T> {\n  const { data, success, error } = config.schema.safeParse(config.dto);\n\n  if (success) {\n    return data;\n  } else {\n    captureError(`API Validation Error: ${config.schemaName}`, {\n      dto: config.dto,\n      error: error.message,\n      issues: error.issues,\n    });\n\n    throw error;\n  }\n}\n\nfunction captureError(message: string, extra = {}): void {\n  if (__DEV__) {\n    console.error(message, extra);\n  } else {\n    // TODO: report to Sentry/something else\n  }\n}\n```\n\nThis helper function takes a data transfer object (DTO), a Zod schema, and a schema name as arguments. It then validates the DTO against the schema and returns the validated data if validation succeeds. If validation fails, it logs an error message and throws an error.\n\nWith such a function ready, we only have to define a schema for each API response and use the helper function to validate the response data. Here's an example of how that could look like:\n\n```ts\n// api/requests/v1/accountDetails/schema.ts\nimport { z } from \"zod\";\n\nexport const schema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  login: z.string(),\n  createdAt: z.string().datetime(),\n  deletedAt: z.string().datetime().nullable(),\n});\n```\n\n```ts\n// api/requests/v1/accountDetails/types.ts\nimport { z } from \"zod\";\nimport { schema } from \"./schema\";\n\nexport type AccountDetailsResponse = z.infer<typeof schema>;\nexport type AccountDetailsErrorResponse = Record<string, unknown>;\n```\n\n```ts {8-10}\n// api/requests/v1/accountDetails/request.ts\nimport { apiClient } from \"@/api/client\";\nimport { validateSchema } from \"@/api/validator\";\n\nimport { schema } from \"./schema\";\nimport { AccountDetailsResponse } from \"./types\";\n\nfunction validate(dto: unknown): AccountDetailsResponse {\n  return validateSchema({ dto, schema, schemaName: \"v1/account/details\" });\n}\n\nexport async function getAccountDetails(): Promise<AccountDetailsResponse> {\n  const response = await apiClient.get(\"/api/v1/account/details\");\n\n  return validate(response.data);\n}\n```\n\n## Conclusion\n\nAPI response validation is a critical aspect of building reliable and robust applications. Zod, with its TypeScript-first approach and expressive syntax, simplifies the process of defining and enforcing data schemas.\n\nBy incorporating Zod into your workflow, you can enhance the integrity of your APIs, catch potential issues early, and ensure that your application communicates seamlessly with external services.\n","tableOfContents":{"items":[{"url":"#what-is-zod","title":"What is Zod"},{"url":"#why-use-zod","title":"Why use Zod"},{"url":"#how-to-use-zod","title":"How to use Zod"},{"url":"#conclusion","title":"Conclusion"}]},"fields":{"slug":"/blog/2023-11-19-api-response-validation-with-zod/","timeToRead":{"minutes":3.33}},"frontmatter":{"dateCreated":"November 19, 2023","dateUpdated":"November 19, 2023","datePublished":"November 19, 2023","title":"API Response validation with Zod","authors":["Bartosz Łaniewski"],"language":"en","keywords":["JavaScript","TypeScript"],"description":null},"internal":{"contentFilePath":"/home/runner/work/bartozzz.github.io/bartozzz.github.io/content/blog/2023-11-19-api-response-validation-with-zod/index.md"}},{"id":"afb5b5a2-7917-5f76-bc42-4912cf350058","excerpt":"There are more than 31000 repositories on GitHub using Flow but only a few ones export Flow definitions. In this article, we will see how to…","body":"\nThere are more than [31000 repositories on GitHub][1] using Flow but only a few ones export Flow definitions. In this article, we will see how to export Flow definitions for a module. Before getting started, I encourage you to read my previous article about „_[Publishing tree shaking friendly packages to npm](/javascript/2018/04/29/publishing-packages-to-npm.html)_”.\n\n## What is Flow?\n\nYou've probably already seen [Gary's talk from CodeMash 2012](https://www.destroyallsoftware.com/talks/wat) about JavaScript. With Flow in your hands, there should be no more „_wat's_”.\n\n>Flow is a static type checker for your JavaScript code. It does a lot of work to make you more productive. Making you code faster, smarter, more confidently, and to a bigger scale. – [Introduction to type checking with Flow](https://flow.org/en/docs/getting-started/)\n\nFlow was first presented at the [Scale Conference](https://atscaleconference.com/) in 2014 by Facebook. Its main goal was to add additional syntax to JavaScript language to prevent type errors and give developers a more concise idea about bugs that can occur in their code. It also gives a way for IDEs to provide a better environment for spotting errors in real-time.\n\n## How to export Flow definitions?\n\nThere are currently two ways of exporting Flow definitions. The easiest one is by exporting `.js.flow` files within your package. However, if you are planning on versioning your definitions, you should contribute to `flow-typed` repository instead.\n\n### Exporting Flow files\n\nAll you have to do is copy source files (i.e. not transpiled by _Flow_ and _Babel_) to `.js.flow` format and include them with the library. Let's start by installing required npm dependencies using the following command:\n\n```bash\n$ npm install --save-dev glob fs-extra\n```\n\nOnce installed, we can write a script which will copy all source files to the `/dest` directory and change their extension to `.js.flow`:\n\n```javascript\n// File: ./bin/defs.js\nimport { basename, resolve } from \"path\";\nimport { copy } from \"fs-extra\";\nimport glob from \"glob\";\n\nasync function copyFile(file) {\n  const srcDir  = resolve(__dirname, \"../src\");\n  const destDir = resolve(__dirname, \"../dest\");\n  const fileDef = `${file}.flow`.replace(srcDir, destDir);\n\n  await copy(file, fileDef);\n\n  console.log(`Copied ${file} to ${fileDef}`);\n}\n\nglob(resolve(__dirname, \"../src/**/*.js\"), (err, files) => {\n  files.forEach(file => copyFile(file));\n});\n```\n\nThen, you can execute this script at the end of your build pipeline, as follows:\n\n```json\n{\n  \"scripts\": {\n    \"build\": \"npx babel ./src -d ./dest && npm run defs\",\n    \"watch\": \"npx babel ./src -d ./dest --watch\",\n    \"defs\": \"npx babel-node ./bin/defs.js\"\n  },\n\n  \"devDependencies\": {\n    \"babel-cli\": \"^6.26.0\"\n  }\n}\n```\n\n### Adding definitions to flow-typed\n\nFlow supports library definitions which allow you to describe the interface of a module or library separate from the implementation of that module/library. You can add your own definitions by [contributing library definitions][3] which reside in `flow-typed` repository. It will allow people who use your library to fetch definitions by using the following command:\n\n```bash\n$ flow-typed install library@x.x.x\n```\n\n## Conclusion\n\nIt is important to export Flow definitions so Flow can give errors if someone accidentally miss-use your library. Additionally, it integrates well with most IDEs, as it gives developers a better experience by providing documentation, auto-complete and errors in real-time.\n\n[1]: https://github.com/facebook/flow/network/dependents\n[2]: https://github.com/flowtype/flow-typed/wiki/FAQs\n[3]: https://github.com/flowtype/flow-typed/wiki/Contributing-Library-Definitions\n","tableOfContents":{"items":[{"url":"#what-is-flow","title":"What is Flow?"},{"url":"#how-to-export-flow-definitions","title":"How to export Flow definitions?","items":[{"url":"#exporting-flow-files","title":"Exporting Flow files"},{"url":"#adding-definitions-to-flow-typed","title":"Adding definitions to flow-typed"}]},{"url":"#conclusion","title":"Conclusion"}]},"fields":{"slug":"/blog/2018-05-13-exporting-flow-definitions-to-npm/","timeToRead":{"minutes":2.545}},"frontmatter":{"dateCreated":"May 12, 2018","dateUpdated":"May 12, 2018","datePublished":"May 12, 2018","title":"Exporting Flow definitions in npm packages","authors":["Bartosz Łaniewski"],"language":"en","keywords":["JavaScript","Flow","npm"],"description":null},"internal":{"contentFilePath":"/home/runner/work/bartozzz.github.io/bartozzz.github.io/content/blog/2018-05-13-exporting-flow-definitions-to-npm/index.md"}},{"id":"6198e53d-3784-5a9e-8e80-e40f4f623c58","excerpt":"With the rise of ES2015, modules have officially become an integral part of JavaScript. By their nature, ES2015 modules are static and can…","body":"\nWith the rise of ES2015, modules have officially become an integral part of JavaScript. By their nature, ES2015 modules are static and can get optimized at the compile time. Various tools and techniques have been created to minimize the total size of generated bundles. The one described in this article is called tree shaking.\n\n## Introduction to npm packages\n\n**npm** is the most popular package manager for JavaScript. It is shipped by default with Node.js – the JavaScript runtime environment. Each month, there are over [20 billions downloads](https://www.npmjs.com/) from the npm registry which counts more than half a million different packages.\n\n### Basic terminology & overview\n\nA package is a directory described by a `package.json`. Each package is composed of one or more modules which can be loaded by Node.js' `require()`. In most cases, a package will expose a single module via the `main` field specified in `package.json`. If there's no such field, npm will look for `index.js` in the root directory.\n\nIn the era of JavaScript bundlers such as [Webpack](https://webpack.js.org/) or [Browserify](http://browserify.org/), the idea of a single entry-point is strongly encouraged. In fact, most of the bundlers will output a unique JavaScript file containing all built modules. For example, let's consider the following code:\n\n```javascript\n// Source file: index.js\nexport * as moduleA from \"./src/moduleA\";\nexport * as moduleB from \"./src/moduleB\";\nexport * as moduleC from \"./src/moduleC\";\n```\n\nTo build this package, Webpack will start by compiling the source file (called _entry point_). It will then move from `import` to `import` and include every required file in the build pipe. It will generate a single bundle containing all the required modules, as follows:\n\n```javascript\n(function (modules) {\n  // Webpack stuff\n})({\n  \"./index.js\": function (module, exports, __webpack_require__) {\n    \"use strict\";\n    // Compiled entry-point\n  },\n  \"./src/moduleA.js\": function (module, exports, __webpack_require__) {\n    \"use strict\";\n    // Compiled moduleA\n  },\n  \"./src/moduleB.js\": function (module, exports, __webpack_require__) {\n    \"use strict\";\n    // Compiled moduleB\n  },\n  \"./src/moduleC.js\": function (module, exports, __webpack_require__) {\n    \"use strict\";\n    // Compiled moduleC\n  },\n});\n```\n\nOnce published to npm, each module can be loaded using Node.js' `require()` (or [ES2015 `import`](https://tc39.github.io/ecma262/#sec-imports)) as follows:\n\n```javascript\nimport { moduleA, moduleB, moduleC } from \"package\";\n\nmoduleA.internal();\nmoduleB.internal();\nmoduleC.internal();\n```\n\nThe reason why such bundlers can work, is that [ES2015 packages are static by nature][1]: you can predict which modules are being imported and exported just by analysing the code, without the need to execute it. However, this has some drawbacks:\n\n- **conditional imports and exports** are unsupported – you have to declare your imports at the top-level;\n- both imports and exports **cannot have any dynamic parts** – you cannot use string concatenation in `require()`.\n\n### The role of tree shaking\n\nTree shaking simply means _dead code elimination_ – a script will perform code analysis for a given bundle and check at the **compile (build) time** which modules are never being used. Let’s take the previous example:\n\n```javascript\nimport { moduleA, moduleB } from \"package\";\n```\n\n`package` exports `moduleA`, `moduleB` and `moduleC` but only the first two are required. Without tree shaking, the final bundle would be a lot bigger since it would contain unreachable code. During bundling, unused exports can be removed, potentially resulting in significant space savings.\n\n> Utilizing the tree shaking and dead code elimination can significantly reduce the code size we have in our application. The less code we send over the wire the more performant the application will be. – [Alex Bachuk](https://medium.com/@netxm/what-is-tree-shaking-de7c6be5cadd).\n\n## Creating tree shaking friendly packages\n\nOur goal will be to expose multiple module bundles from a single package, so ones can `import` only necessary parts instead of the entire library:\n\n```javascript\nimport * as moduleA from \"package/moduleA\";\nimport * as moduleB from \"package/moduleB\";\nimport { funcA, funcB } from \"package/moduleC\";\n```\n\n- If we import only `moduleA`, the two others modules will not be included in the final bundle because they aren't required anywhere.\n- If we import only a specific function from a module (ex. `funcA`), the rest of the module's content will be ignored.\n\nAdditionally, we don't have to access a named export like in previous examples. That means we don't have to do a slow, dynamic property lookup. In our case, we statically know the content and can optimize the access.\n\n### Directory structure overview\n\n```\n├── scripts/\n│   └── copy.js\n├── package/\n│   └── package.json\n├── source/\n│   ├── moduleA/\n│   ├── moduleB/\n│   ├── moduleC/\n│   └── index.js\n├── README.md\n├── LICENSE\n└── package.json\n```\n\n- `scripts/` will contain all JavaScript binaries that will be used to build your package. Those steps will be automated via `package.json` scripts.\n- `package/` directory is where your package will actually reside once it is compiled. This is the directory that will be pushed to the npm registry.\n- `source/` directory is where your package source resides. It will not be pushed to the npm registry, but should be contained in the repository.\n\nAs you can see, there are two `package.json` files. The one at the root will be used to declare your `dependencies`, basic package data and build scripts. The second one is declaring in details the package that will be pushed to the npm registry.\n\n### Compiling and building the package\n\nIn this article, we will use [Babel](https://babeljs.io/) for the compilation process. Babel is a JavaScript transpiler that converts ECMAScript and other JavaScript subsets into plain JavaScript that can be used in any environment. First, you need to install Babel as a development dependency in your project:\n\n```bash\n$ npm install --save-dev babel-cli\n```\n\nFor full installation details, I encourage you to check [Babel' setup section in their documentation](https://babeljs.io/docs/setup/). Once Babel is installed, we can define a few scripts in `/package.json`:\n\n```json\n{\n  \"private\": true,\n  \"dependencies\": {\n    \"rimraf\": \"^2.6.2\"\n  },\n  \"scripts\": {\n    \"prepare\": \"npm start\",\n    \"start\": \"npm run clean && npm run build && npm run copy\",\n    \"clean\": \"npx rimraf ./package/*\",\n    \"build\": \"npx babel ./source --out-dir ./package\",\n    \"copy\": \"npx babel-node ./scripts/copy.js\"\n  }\n}\n```\n\n- `npm run clean` will remove built modules from `/package` directory.\n- `npm run build` will build modules and pipe the bundles to `/package` directory.\n- `npm run copy` will execute `/scripts/copy.js` script described in the next section.\n\nNote that it is important to set `\"private\": true` in `/package.json`. It will prevent you from accidentally pushing your entire entire repository to the npm registry instead of only the builded modules.\n\n### Copying required files into package\n\nThe script below will copy important files such as `README.md` and `LICENSE` to your final package. Additionally, it will create a brand new `package.json`.\n\n```javascript\n// File: /scripts/copy.js\nimport { basename, resolve } from \"path\";\nimport { copy, writeFile } from \"fs-extra\";\n\nasync function copyFile(file) {\n  const fileName = basename(file);\n  const filePath = resolve(__dirname, \"../package/\", fileName);\n\n  await copy(file, filePath);\n\n  console.log(`Copied ${file} to ${filePath}`);\n}\n\nasync function createPackageFile() {\n  const oldPckgPath = resolve(__dirname, \"../package.json\");\n  const oldPckgData = require(oldPckgPath);\n\n  delete oldPckgData.private;\n  delete oldPckgData.scripts;\n  delete oldPckgData.devDependencies;\n\n  const newPckgPath = resolve(__dirname, \"../package/package.json\");\n  const newPckgData = Object.assign(oldPckgData, { main: \"./index.js\" });\n  await writeFile(newPckgPath, JSON.stringify(newPckgData), \"utf8\");\n\n  console.log(`Created package.json in ${newPckgPath}`);\n}\n\nasync function run() {\n  await [\"README.md\", \"LICENSE\"].map((file) => copyFile(file));\n  await createPackageFile();\n}\n\nrun();\n```\n\n## Limitations and solutions\n\nTree shaking is a relatively new technology and still have some limitations. While not every single one can be resolved directly in tree shaking, there are various ways around these problems.\n\n### Side effects in module bundles\n\nSome modules have side effects: they can perform additional tasks such as modifying global variables instead of just exporting their content. According to the ECMAScript specifications, all child modules must be evaluated because they could contain side effects. Let's take the following examples:\n\n```javascript\n// moduleA\nconsole.log(\"Side effect\");\n\nexport {/* … */};\nexport default /* … */;\n```\n\n```javascript\n// moduleB\nwindow.a = /* … */;\nwindow.b = /* … */;\nwindow.c = /* … */;\n```\n\nBecause of this potential behaviour, tree shaking cannot remove all unreachable code. However, some bundlers, such as Webpack, drop the responsibility on the developers by providing a `sideEffect` option. By setting this flag to `false`, you indicate that your package is a _pure module_ and doesn't have any side effects. Therefore, it can get aggressively optimized.\n\n### Wrong specs implementation\n\n> Current tooling differs on the correct way to handle default imports/exports. Avoiding them all together can help avoid tooling bugs and conflicts. – [TSLint rules][6]\n\n### Class-based tree shaking\n\nClass-based tree shaking is currently not supported because of the dynamic nature of JavaScript's property accessors – they cannot be statically determined, especially when using the bracket notation. Let's consider the following example:\n\n```javascript\nconst bar = new Foo();\n\nbar.methodA();\nbar[\"methodB\"]();\nbar[`method${n}`]();\nbar[\"methodD\".split(\"\").reverse().join(\"\")]();\n```\n\nAs you can see, `methodA` and `methodB` can be statically determined as being used at compile time, but this is not be the case for the last two cases. A potential solution to this problem could be to import methods separately and call them with an instance:\n\n```javascript\nimport Foo, { methodA, methodB } from \"foo\";\n\nconst bar = new Foo();\nmethodA.call(bar, \"param\");\nmethodB.call(bar, \"param\");\n```\n\nThis doesn't solve cases like `methodD` in the previous example but, at least, can be optimized by tree shaking. There are different proposals which could serve this purpose a little bit better.\n\n#### Bind operator proposal\n\n> The :: operator creates a bound function such that the left hand side of the operator is bound as the this variable to the target function on the right hand side. By providing syntactic sugar for these use cases we will enable a new class of \"virtual method\" library, which will have usability advantages over the standard adapter patterns in use today. – [tc39/proposal-bind-operator][2]\n\n```javascript\nimport Foo, { methodA, methodB } from \"foo\";\n\nconst bar = new Foo();\nbar::methodA();\nbar::methodB();\n```\n\n#### Pipeline operator proposal\n\n> This proposal introduces a new operator \\|\\> similar to _F#, OCaml, …, Hack and LiveScript_, as well as UNIX pipes. It's a backwards-compatible way of streamlining chained function calls in a readable, functional manner, and provides a practical alternative to extending built-in prototypes. – [tc39/proposal-pipeline-operator][3]\n\n```javascript\nimport Foo, { methodA, methodB } from \"foo\";\n\nconst bar = new Foo();\nbar |> methodA();\nbar |> methodB();\n```\n\n## Resources\n\n1. [Exploring JS – Static module structure][1]\n2. [ECMAScript – This-Binding Syntax][2]\n3. [ECMAScript – The Pipeline Operator][3]\n4. [Rollup – Tree shaking documentation][4]\n5. [Webpack – Tree shaking documentation][7]\n6. [Rollup vs Webpack2 – David Rodenas][5]\n\n[1]: http://exploringjs.com/es6/ch_modules.html#static-module-structure \"Exploring JS – Static module structure\"\n[2]: https://github.com/tc39/proposal-bind-operator \"ECMAScript This-Binding Syntax\"\n[3]: https://github.com/tc39/proposal-pipeline-operator \"ESNext Proposal: The Pipeline Operator\"\n[4]: https://rollupjs.org/guide/en#tree-shaking \"Rollup – Tree shaking documentation\"\n[5]: http://david-rodenas.com/posts/rollup-vs-webpack-and-tree-shaking \"Rollup vs. Webpack – Tree shaking\"\n[6]: https://palantir.github.io/tslint/rules/no-default-export/ \"TSLint rules – no default export\"\n[7]: https://webpack.js.org/guides/tree-shaking/ \"Webpack – Tree shaking documentation\"\n","tableOfContents":{"items":[{"url":"#introduction-to-npm-packages","title":"Introduction to npm packages","items":[{"url":"#basic-terminology--overview","title":"Basic terminology & overview"},{"url":"#the-role-of-tree-shaking","title":"The role of tree shaking"}]},{"url":"#creating-tree-shaking-friendly-packages","title":"Creating tree shaking friendly packages","items":[{"url":"#directory-structure-overview","title":"Directory structure overview"},{"url":"#compiling-and-building-the-package","title":"Compiling and building the package"},{"url":"#copying-required-files-into-package","title":"Copying required files into package"}]},{"url":"#limitations-and-solutions","title":"Limitations and solutions","items":[{"url":"#side-effects-in-module-bundles","title":"Side effects in module bundles"},{"url":"#wrong-specs-implementation","title":"Wrong specs implementation"},{"url":"#class-based-tree-shaking","title":"Class-based tree shaking"}]},{"url":"#resources","title":"Resources"}]},"fields":{"slug":"/blog/2018-04-29-publishing-packages-to-npm/","timeToRead":{"minutes":8.86}},"frontmatter":{"dateCreated":"April 28, 2018","dateUpdated":"April 28, 2018","datePublished":"April 28, 2018","title":"Publishing tree shaking friendly npm packages","authors":["Bartosz Łaniewski"],"language":"en","keywords":["JavaScript","npm"],"description":null},"internal":{"contentFilePath":"/home/runner/work/bartozzz.github.io/bartozzz.github.io/content/blog/2018-04-29-publishing-packages-to-npm/index.md"}}]}},"staticQueryHashes":["3216310583","3764592887","991007626"],"slicesMap":{}}